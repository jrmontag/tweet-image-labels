{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image prediction accuracy analysis\n",
    "\n",
    "- Josh Montague (MIT License)\n",
    "\n",
    "In this notebook, we'll look at the TSV dump from the mysql db that recorded the prediction accuracies (top1, top5, none) from the webapp.\n",
    "\n",
    "Note: the dialog and some of the choices were specific to my data. The outputs should be sufficiently general that they work with your data, though. Feel free to modify the text and conclusions if it bothers you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('bmh')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project_dir = '/path/to/repo/'\n",
    "\n",
    "# we don't need the id or URL for this\n",
    "results = (pd.read_csv(project_dir + 'rdata/db-results-export.tsv', sep='\\t')\n",
    "           .drop(['id','link'], axis=1)\n",
    "           )\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure they're in time order (snowflake id encodes timing)\n",
    "results = results.sort_values('tweet_id', ascending=True)\n",
    "#results.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# distribution of labels?\n",
    "results['label_id'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that:\n",
    "\n",
    "```\n",
    "0 = top1\n",
    "1 = top5\n",
    "2 = None\n",
    "```\n",
    "\n",
    "Also recall that the `3`s are from a bug in the UI buttons, and should be `2`s. I think there were < 10 total uses of the buttons, so there are probably a handful of 2s that are 1s and 1s that are 0s. But since they're one to two OOM less, who cares.\n",
    "\n",
    "Just drop the 3s for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = results.query('label_id != 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results['label_id'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick (and unsurprising) take: most of the predictions are incorrect. \n",
    "\n",
    "Given the random things people post on Twitter, this was expected from the outset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze score distributions\n",
    "\n",
    "Let's start by looking at the scores and their distributions. Start by recalling the format of the data.\n",
    "\n",
    "We can begin by simply looking at the distributions of the prediction probabilities. Recall that these are the ordered (1 to 5), most-probable predictions for each image. \n",
    "\n",
    "For annoying historical reasons, the remainder of the notebook uses a dataframe named `adj_results`, so make that connection here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adj_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adj_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = (adj_results[['score1','score2','score3','score4','score5']]\n",
    "        .plot.hist(bins=100, \n",
    "                   subplots=True, \n",
    "                   #sharey=True, \n",
    "                   figsize=(8,8))\n",
    "       )\n",
    "\n",
    "plt.xlabel('prediction probability')\n",
    "axes[0].set_title('distribution of top-5 prediction probabilities ({} images)'.format(len(adj_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These distributions are consistent with the notion that `score1` is always the most probable result, `score2` less so, and so on. There is never a case in which the fifth-most likely label is predicted to be highly likely.\n",
    "\n",
    "Interestingly, note that most of the distribution weight for `score1` (the most probably prediction) is still at very low probability. This indicates that the model is generally not confident in it's predictions for our data set. Another way of saying it is that this out-of-the-box model was not designed for the task of labeling random twitter images, and as such it doesn't do a spectacular job of it. \n",
    "\n",
    "But that's fine, and expected. We're looking to see if we can still get some use out of it.\n",
    "\n",
    "If you uncomment the `sharey` kwarg, you can also see more clearly that all the probability masses do sum to 1, but the `score1` distribution is just much more spread out over the probability range.\n",
    "\n",
    "Note that this view doesn't account for (or display) the ground truth that we applied. Let's encode that in the frame with a text label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a column that maps the label_id to the text labels \n",
    "label_dict = {0:'top1', 1:'top5', 2:'none'}\n",
    "\n",
    "adj_results['truth'] = adj_results['label_id'].apply(lambda x: label_dict[x])\n",
    "\n",
    "adj_results[['label_id','truth']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction probability distributions per label\n",
    "\n",
    "Now that we have the labels, let's use them in a similar set of distributions as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adj_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['score1','score2','score3','score4','score5']\n",
    "labels = ['top1','top5','none']\n",
    "\n",
    "# make 3 separate charts (one for each text label) \n",
    "#  each one will have subplots for the 5 scores\n",
    "for label in labels:\n",
    "    axes = (adj_results.query('truth == @label')[cols].plot.hist(bins=100,\n",
    "                                                                    subplots=True, \n",
    "                                                                    #sharey=True, \n",
    "                                                                    figsize=(8,6)                                                                    \n",
    "                                                                    )\n",
    "            )\n",
    "    # put the title on the top subplot\n",
    "    axes[0].set_title('score distribution for label={}'.format(label))\n",
    "\n",
    "plt.legend()    \n",
    "plt.xlabel('prediction probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty interesting shapes. Given that the distributions for each label are pretty unique, we could probably train some other model to predict the labels based on all the top 5 scores. Interesting idea, but not the point right now; we want to know more about how to use the `top1` predictions. \n",
    "\n",
    "Of note: in both `top5` and `none`, the `score1` (highest probability prediction) can be as high as 0.9 or more. Similarly, even with the `top1` label, some `score1` values are as low as 0.1. **This makes it clear that we can't blindly use the `score1` probability.**\n",
    "\n",
    "We need something like an ROC curve (FPR v TPR) on `score1` to understand the tradeoffs. \n",
    "\n",
    "## ROC curve\n",
    "\n",
    "How do we construct an ROC curve for this data? [Wiki ref for calculations](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "\n",
    "- assume `score1` is the prediction probability of a binary classifier\n",
    "- assume ground truth is binary: the label is `top1` or it's not \n",
    "- vary probability decision-making threshold from 0 to 1\n",
    "- at each threshold\n",
    "    - everything above threshold is \"prediction positive\"\n",
    "    - everything below threshold is \"prediction negative\"\n",
    "- at all times (fixed)\n",
    "    - actual data with `top1` / \"1\" labels are the \"condition positive\"\n",
    "    - everything else is \"condition negative\"\n",
    "- TPR = TP / cond pos\n",
    "- FPR = FP / cond neg\n",
    "- plot TPR vs. FPR at each threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_df = adj_results[['score1','truth']].copy(deep=True)\n",
    "\n",
    "# for easier comparisons, let's convert 'top1' to 1 and everything else to 0\n",
    "sub_df['truth'] = sub_df['truth'].apply(lambda x: 1 if x == 'top1' else 0)\n",
    "\n",
    "#del sub_df['pred_acc']\n",
    "sub_df.rename(columns={'score1':'pred'}, inplace=True)\n",
    "\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# threshold range\n",
    "t_rng = np.linspace(0,1,100)\n",
    "\n",
    "# ground truth (fixed)\n",
    "cond_pos = len(sub_df.query('truth == 1'))\n",
    "cond_neg = len(sub_df.query('truth == 0'))\n",
    "\n",
    "# use this copy of the frame above for flipping the preds at each threshold\n",
    "# `tmp_df` will be our binary label holder\n",
    "tmp_df = sub_df.copy(deep=True)\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "\n",
    "for t in t_rng:\n",
    "    # flip to 0 or 1 based on this threshold\n",
    "    tmp_df['pred'] = sub_df['pred'].apply(lambda x: 1 if x > t else 0)\n",
    "    # calculate TPR (TP / cond pos)\n",
    "    tp = len(tmp_df.query('pred == 1 and truth == 1'))\n",
    "    tpr = tp / cond_pos\n",
    "    tpr_list.append(tpr)\n",
    "    # calculate FPR (FP / cond neg)\n",
    "    fp = len(tmp_df.query('pred == 1 and truth == 0'))\n",
    "    fpr = fp / cond_neg\n",
    "    fpr_list.append(fpr)\n",
    "\n",
    "#print('tpr_list: ', tpr_list)\n",
    "#print('fpr_list: ', fpr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.plot(fpr_list, tpr_list, '--.', label='model')\n",
    "plt.plot(t_rng, t_rng, ':', c='k', label='diagonal')\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "# rough estimate of auc\n",
    "auc = sum(tpr_list)/len(tpr_list)\n",
    "plt.title('ROC curve for top1 predictions (AUC ~ {:.2f})'.format(auc))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step would be to choose a threshold value for the prediction probability and then say that's where we operate. How do we choose the best threshold?\n",
    "\n",
    "Some googling suggests there the \"optimal\" choice is typically dependent on the industry and context of the problem. That said, [some texts](https://books.google.com/books?id=JzT_CAAAQBAJ&lpg=PT43&dq=Selecting%20an%20Optimal%20Threshold%20ROC%20curve&pg=PT43#v=onepage&q=Selecting%20an%20Optimal%20Threshold%20ROC%20curve&f=false) suggest there are two common answers:\n",
    "- the point that is closest to \"ideal\" (i.e. (0,1))\n",
    "- the point that is furthest from the \"informationless diagonal\"\n",
    "\n",
    "Let's calculate both and see what they look like.\n",
    "\n",
    "### Closest to \"ideal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create an array of the TPR and FPR values\n",
    "fptp_list = []\n",
    "for f,t in zip(fpr_list, tpr_list):\n",
    "    fptp_list.append([f, t])\n",
    "\n",
    "# NB: fptp starts at (1,1) at t=0; reverse order (start at (0,0)) for \n",
    "#  same convention as t_rng  \n",
    "fptp = np.array(fptp_list)[::-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create an array of the \"ideal\" values\n",
    "ideal_list = []\n",
    "for _ in fptp:\n",
    "    ideal_list.append([0,1])\n",
    "\n",
    "ideal = np.array(ideal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate pair-wise (euclidean) distances\n",
    "ideal_dist = np.array([np.linalg.norm(a-b) for a,b in zip(ideal, fptp)])\n",
    "\n",
    "# get the index of the smallest distance\n",
    "ideal_position = np.argmin(ideal_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ideal_dist should be 1.0 at both ends!\n",
    "#plt.plot(ideal_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Furthest from diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create an array of the diagnoal values\n",
    "# NB: diag starts at (0,0)\n",
    "diag = np.array([[x, x] for x in t_rng])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate pair-wise (euclidean) distances\n",
    "# (reverse fptp since it starts goes upper right => lower left)\n",
    "#diag_dist = np.array([np.linalg.norm(a-b) for a,b in zip(diag, fptp[::-1])])\n",
    "diag_dist = np.array([np.linalg.norm(a-b) for a,b in zip(diag, fptp)])\n",
    "\n",
    "# get the index of the largest distance\n",
    "diag_position = np.argmax(diag_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# diag_dist should be 0 at both ends!\n",
    "#plt.plot(diag_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The threshold closest to the \"ideal\" point in the ROC curve is at:')\n",
    "print(' (FPR, TPR)={}'.format(fptp[ideal_position]))\n",
    "print(' with prediction threshold={:.3f}'.format(t_rng[ideal_position]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The threshold furthest from the \"informationless diagonal\" in the ROC curve is at:')\n",
    "print(' (FPR, TPR)={}'.format(fptp[diag_position]))\n",
    "print(' with prediction threshold={:.3f}'.format(t_rng[diag_position]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.plot(fpr_list, tpr_list, '--.', label='model')\n",
    "plt.plot(t_rng, t_rng, ':', c='k', label='diagonal')\n",
    "\n",
    "# diagonal threshold\n",
    "plt.plot(*fptp[diag_position], 'o', markersize=10, alpha=0.6,\n",
    "         label='furthest from diagonal (t={:.2f})'.format(t_rng[diag_position])\n",
    "        )\n",
    "plt.plot([fptp[diag_position][0], fptp[diag_position][0]],\n",
    "         fptp[diag_position],\n",
    "         'k--', alpha=0.5\n",
    "        )\n",
    "\n",
    "# ideal threshold\n",
    "plt.plot(*fptp[ideal_position], 'o', markersize=10, alpha=0.6,\n",
    "         label='closest to ideal (t={:.2f})'.format(t_rng[ideal_position])\n",
    "         )\n",
    "plt.plot([ideal[0][0], fptp[ideal_position][0]],\n",
    "         [ideal[0][1], fptp[ideal_position][1]],\n",
    "        'k--', alpha=0.5\n",
    "        )\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "# rough estimate of auc\n",
    "auc = sum(tpr_list)/len(tpr_list)\n",
    "plt.title('ROC curve for top1 predictions (AUC ~ {:.2f})'.format(auc))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is some random sampling involved in generating the first region of data, these threshold values can vary a bit. In repeated runs, they typically range from 0.5 to 0.6, and sometimes they are the same point.\n",
    "\n",
    "Let's use the \"furthest from the diagonal\" point and move on to recalculate other parts of the confusion matrix params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set threshold\n",
    "t = t_rng[diag_position]\n",
    "\n",
    "# ground truth still fixed at cond_pos, cond_neg\n",
    "\n",
    "# use this copy of the frame above for flipping the preds at each threshold\n",
    "tmp_df = sub_df.copy(deep=True)\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "\n",
    "# flip to 0 or 1 based on this threshold\n",
    "tmp_df['pred'] = sub_df['pred'].apply(lambda x: 1 if x > t else 0)\n",
    "# calculate TPR (TP / cond pos)\n",
    "tp = len(tmp_df.query('pred == 1 and truth == 1'))\n",
    "tpr = tp / cond_pos\n",
    "tpr_list.append(tpr)\n",
    "# calculate FPR (FP / cond neg)\n",
    "fp = len(tmp_df.query('pred == 1 and truth == 0'))\n",
    "fpr = fp / cond_neg\n",
    "fpr_list.append(fpr)\n",
    "# calculate TN + FN\n",
    "tn = len(tmp_df.query('pred == 0 and truth == 0'))\n",
    "fn = len(tmp_df.query('pred == 0 and truth == 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# accuracy \n",
    "accuracy = (tp + tn) / len(tmp_df)\n",
    "\n",
    "print('total accuracy: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common categories of predictions\n",
    "\n",
    "Ok, so we've got a mid- to high-70s% accuracy image classifier by setting the threshold (reflected in `tmp_df`), but it's possible that there are class imbalances or other sorts of bias in the *types* of images we can accurately predict. \n",
    "\n",
    "What are the most common accurately predicted images, and what fraction of the data do they comprise?\n",
    "\n",
    "We can use the index of tmp_df to join back onto the larger frame with the actual labels.\n",
    "\n",
    "**Most common TP labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tmp_df still has our binary labels\n",
    "\n",
    "# most common labels for TP\n",
    "(tmp_df.query('pred == 1 and truth == 1')\n",
    " #.head()\n",
    " .join(adj_results[['tweet_id','keyword1']], how='left')\n",
    " .groupby(by='keyword1')\n",
    " .count()\n",
    " .sort_values('tweet_id', ascending=False)[['tweet_id']]\n",
    " .rename(columns={'tweet_id':'count'})\n",
    " .head(15)\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we see some data that feels consistent with my experience hand-labeling things: the model is really good at recognizing suits and websites in images. Recall that in my hand-labeling, I counted anything that was a mobile app or deskstop screenshot as \"web_site\". Then, however, the counts drop off pretty quickly.\n",
    "\n",
    "**Most common FP categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tmp_df still has our binary labels\n",
    "\n",
    "# most common labels for FP\n",
    "(tmp_df.query('pred == 1 and truth == 0')\n",
    " #.head()\n",
    " .join(adj_results[['tweet_id','keyword1']], how='left')\n",
    " .groupby(by='keyword1')\n",
    " .count()\n",
    " .sort_values('tweet_id', ascending=False)[['tweet_id']]\n",
    " .rename(columns={'tweet_id':'count'})\n",
    " .head(15)\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a huge shock, there are a bunch of screenshots, so web_site also is at the top of the false positive list. I'm not sure what to make of envelope and menu making it into the top of that list, either.\n",
    "\n",
    "**Most common FN categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tmp_df still has our binary labels\n",
    "\n",
    "# most common labels for FN\n",
    "(tmp_df.query('pred == 0 and truth == 1')\n",
    " #.head()\n",
    " .join(adj_results[['tweet_id','keyword1']], how='left')\n",
    " .groupby(by='keyword1')\n",
    " .count()\n",
    " .sort_values('tweet_id', ascending=False)[['tweet_id']]\n",
    " .rename(columns={'tweet_id':'count'})\n",
    " .head(15)\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most common TN categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tmp_df still has our binary labels\n",
    "\n",
    "# most common labels for TN\n",
    "(tmp_df.query('pred == 0 and truth == 0')\n",
    " #.head()\n",
    " .join(adj_results[['tweet_id','keyword1']], how='left')\n",
    " .groupby(by='keyword1')\n",
    " .count()\n",
    " .sort_values('tweet_id', ascending=False)[['tweet_id']]\n",
    " .rename(columns={'tweet_id':'count'})\n",
    " .head(15)\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "So, is this image classifier useful? Maybe. It's pretty noisy, though. I do think there's probably utility in attaching labels to tweets when above, say, one of these thresholds. \n",
    "\n",
    "To see what you think: \n",
    "- run the cell below\n",
    "- copy/paste the link and compare to the corresponding `keyword1`\n",
    "    - (it's ok that it has my username, it works anyway!)\n",
    "\n",
    "Re-run the cell as many times as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for true positives\n",
    "tw_id, kw1 = (tmp_df.query('pred == 1 and truth == 1')\n",
    "# for false positives\n",
    "#tw_id, kw1 = (tmp_df.query('pred == 1 and truth == 0')\n",
    "                 .join(adj_results[['tweet_id','keyword1']], how='left')\n",
    "                 .sample(n=1)[['tweet_id','keyword1']].values[0]\n",
    "                 )\n",
    "\n",
    "print('prediction: {}\\n'.format(kw1))\n",
    "print('URL to copy-paste: {}'.format('https://www.twitter.com/jrmontag/status/{}'.format(tw_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Follow up\n",
    "\n",
    "There are many places that this work could go. \n",
    "\n",
    "### similar, better labels\n",
    "\n",
    "For one, now that all of these pieces exist, it would be straightforward (one line of code) to swap out the VGG16 model for [any other pre-trained model](https://keras.io/applications/#available-models). As seen in [this diagram](https://culurciello.github.io/tech/2016/06/04/nets.html) the top1 performance varies (generally slightly higher than VGG16), but other models may also be faster to evaluate according to that post.\n",
    "\n",
    "If the task was done similarly (top1, top5, none), then all of this code could be re-used, as-is (make a new db table, though). The turn around time for a next round (like this one) would be significantly less. Ballpark, based on my time logs:\n",
    "- a couple hours to ensure no other code changes are needed\n",
    "- an overnight (or couple hour) data collection\n",
    "- an overnight (or couple hour) prediction generation\n",
    "- a few hours to label the images\n",
    "- a couple hours to evaluate using this notebook\n",
    "\n",
    "For a total of maybe 10 hours of person time.\n",
    "\n",
    "### image captions\n",
    "\n",
    "Another super intriguing angle to take would be to explore some of the more recent developments in image *captioning* (vs. labeling). There are a handful of examples, most notably [Google's \"Show and Tell\" research](https://research.googleblog.com/2016/09/show-and-tell-image-captioning-open.html). Unfortunately, it doesn't seem like there are open-source weights for this model yet. Perhaps in the near future.\n",
    "\n",
    "### faster analysis\n",
    "\n",
    "One other choice that might make the whole process faster is to skip the notion of \"top5\" results and treat the output as a binary \"top1\" or \"nothing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
